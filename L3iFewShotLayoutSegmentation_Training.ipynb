{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honghanhh/icdar_2024_SAM/blob/main/L3iFewShotLayoutSegmentation_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m57rAj_D6aev",
        "outputId": "cf6ecd1a-f602-4f10-ffc5-9175e14ff935"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf icdar_2024_SAM"
      ],
      "metadata": {
        "id": "YflEYKPE-8zK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Clone the GitHub repository to Colab\n",
        "!git clone https://github.com/honghanhh/icdar_2024_SAM.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wbr4OQW84XZ",
        "outputId": "2cf165b2-8f23-4faf-948e-f12743b04699"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'icdar_2024_SAM'...\n",
            "remote: Enumerating objects: 188, done.\u001b[K\n",
            "remote: Counting objects: 100% (188/188), done.\u001b[K\n",
            "remote: Compressing objects: 100% (181/181), done.\u001b[K\n",
            "remote: Total 188 (delta 18), reused 167 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (188/188), 39.80 MiB | 24.04 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import time\n",
        "import glob\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import random\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import albumentations as A\n",
        "from torch.utils.data import RandomSampler\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "imMDcUAXyrQU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some utility function for process data"
      ],
      "metadata": {
        "id": "IJIe1ce14Skr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convertRGB_to_label(image):\n",
        "    \"\"\"\n",
        "    Convert RGB image to mask label\n",
        "    \"\"\"\n",
        "    # Define RGB color values\n",
        "    colors = {\n",
        "        (0, 0, 0): \"Background\",\n",
        "        (255, 255, 0): \"Paratext\",\n",
        "        (0, 255, 255): \"Decoration\",\n",
        "        (255, 0, 255): \"Main Text\",\n",
        "        (255, 0, 0): \"Title\",\n",
        "        (0, 255, 0): \"Chapter Headings\"\n",
        "    }\n",
        "\n",
        "    # Convert image to numpy array if it's not already\n",
        "    image = np.array(image)\n",
        "\n",
        "    # Convert image to 3D if it's grayscale\n",
        "    if len(image.shape) == 2:\n",
        "        image = np.stack((image,) * 3, axis=-1)\n",
        "\n",
        "    # Initialize labels array with the same shape as the input image\n",
        "    labels = np.zeros_like(image[:, :, 0], dtype=np.int8)\n",
        "\n",
        "    # Assign labels based on color\n",
        "    for color, label in colors.items():\n",
        "        mask = np.all(image == np.array(color), axis=-1)\n",
        "        labels[mask] = list(colors.values()).index(label)\n",
        "\n",
        "    return labels\n",
        "\n",
        "def padding_image(image, divisible):\n",
        "    \"\"\"\n",
        "    Padding image\n",
        "    \"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    pad_h = divisible - (h % divisible)\n",
        "    pad_w = divisible - (w % divisible)\n",
        "    pad_tuple = ((0, pad_h), (0, pad_w)) + ((0, 0),) * (image.ndim - 2)  # Pad along height and width dimensions\n",
        "    padded_image = np.pad(image, pad_tuple, mode='constant')\n",
        "    return padded_image\n",
        "\n",
        "class SlidingWindowCrop(object):\n",
        "    \"\"\"\n",
        "    Class for sliding crop image to given size\n",
        "    \"\"\"\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        new_h, new_w = self.output_size\n",
        "\n",
        "        # Padding if necessary\n",
        "        pad_h = 0 if h % new_h == 0 else new_h - (h % new_h)\n",
        "        pad_w = 0 if w % new_w == 0 else new_w - (w % new_w)\n",
        "        image = np.pad(image, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
        "        mask = np.pad(mask, ((0, pad_h), (0, pad_w)), mode='constant')\n",
        "\n",
        "        cropped_images = []\n",
        "        cropped_masks = []\n",
        "\n",
        "        for i in range(0, h + pad_h, new_h):\n",
        "            for j in range(0, w + pad_w, new_w):\n",
        "                if i + new_h <= h + pad_h and j + new_w <= w + pad_w:\n",
        "                    cropped_images.append(image[i:i+new_h, j:j+new_w])\n",
        "                    cropped_masks.append(mask[i:i+new_h, j:j+new_w])\n",
        "\n",
        "        for k in range(10): #Change it to 40 when training with possible GPU\n",
        "            top = np.random.randint(0, h - new_h)\n",
        "            left = np.random.randint(0, w - new_w)\n",
        "            cropped_images.append(image[top: top + new_h, left: left + new_w])\n",
        "            cropped_masks.append(mask[top: top + new_h, left: left + new_w])\n",
        "        return cropped_images, cropped_masks\n",
        "\n",
        "def get_sampler(dataset, seed=123):\n",
        "    generator = torch.Generator()\n",
        "    generator.manual_seed(seed)\n",
        "    sampler = RandomSampler(dataset, generator=generator)\n",
        "    return sampler"
      ],
      "metadata": {
        "id": "0cryOd8L1GyO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics\n"
      ],
      "metadata": {
        "id": "WOGNv9IrAJIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def f1_score_metric(output, mask):\n",
        "    with torch.no_grad():\n",
        "        f1 = f1_score(mask.flatten().cpu(), output.flatten().cpu(), average='macro')\n",
        "    return f1\n",
        "def pixel_accuracy(output, mask):\n",
        "    with torch.no_grad():\n",
        "        preds = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
        "        num_correct = (preds == mask).sum()\n",
        "        num_pixels = torch.numel(preds)\n",
        "        accuracy = float(num_correct) / float(num_pixels)\n",
        "    return accuracy\n",
        "\n",
        "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=4):\n",
        "    with torch.no_grad():\n",
        "        pred_mask = F.softmax(pred_mask, dim=1)\n",
        "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
        "        pred_mask = pred_mask.contiguous().view(-1)\n",
        "        mask = mask.contiguous().view(-1)\n",
        "\n",
        "        iou_per_class = []\n",
        "        for clas in range(0, n_classes): #loop per pixel class\n",
        "            true_class = pred_mask == clas\n",
        "            true_label = mask == clas\n",
        "\n",
        "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
        "                iou_per_class.append(np.nan)\n",
        "            else:\n",
        "                intersect = (true_class[true_label]).sum().float().item()\n",
        "\n",
        "                union = (true_class + true_label).sum().float().item() - intersect\n",
        "\n",
        "                iou = (intersect + smooth) / (union +smooth)\n",
        "                iou_per_class.append(iou)\n",
        "        return np.nanmean(iou_per_class)"
      ],
      "metadata": {
        "id": "lES8hvNRAMdQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation dataset"
      ],
      "metadata": {
        "id": "OAJ4zJtI4XMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UDIADS(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Training Phase\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            imagePaths,\n",
        "            maskPaths,\n",
        "            transform\n",
        "    ):\n",
        "        self.imagePaths = imagePaths\n",
        "        self.maskPaths = maskPaths\n",
        "        self.trans = transform\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # read data\n",
        "        img_path = self.imagePaths[idx]\n",
        "        mask_path = self.maskPaths[idx]\n",
        "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        h, w = img.shape[0], img.shape[1]\n",
        "        img = 2*((img - img.min()) / (img.max() - img.min())) - 1\n",
        "        mask = cv2.imread(mask_path)\n",
        "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "        #resize_mask = cv2.resize(mask, (int(h/2),int(w/2)), interpolation = cv2.INTER_AREA)\n",
        "        #resize_mask = padding_image(resize_mask, 32)\n",
        "\n",
        "        mask = convertRGB_to_label(mask)\n",
        "        #resize_img = cv2.resize(img, (int(h/2),int(w/2)), interpolation = cv2.INTER_LINEAR)\n",
        "        #resize_img = padding_image(resize_img, 32)\n",
        "\n",
        "\n",
        "        if self.trans:\n",
        "            img, mask = self.trans(img, mask)\n",
        "        img = torch.stack([torch.from_numpy(i) for i in img])\n",
        "        mask = torch.stack([torch.from_numpy(i).long() for i in mask])\n",
        "        img = img.permute(0,3,1,2)\n",
        "\n",
        "\n",
        "        return img, mask, (h,w)#, #repeated_img, repeated_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imagePaths)"
      ],
      "metadata": {
        "id": "FPavK_6V06in"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UDIADS_Validation(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for simple Evaluation and Testing\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            imagePaths,\n",
        "            maskPaths,\n",
        "\n",
        "    ):\n",
        "        self.imagePaths = imagePaths\n",
        "        self.maskPaths = maskPaths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # read data\n",
        "        img_path = self.imagePaths[idx]\n",
        "        mask_path = self.maskPaths[idx]\n",
        "        read_img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        h, w = read_img.shape[0], read_img.shape[1]\n",
        "        img = 2*((read_img - read_img.min()) / (read_img.max() - read_img.min())) - 1\n",
        "        mask = cv2.imread(mask_path)\n",
        "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "        mask = convertRGB_to_label(mask)\n",
        "\n",
        "\n",
        "        #To tensor\n",
        "        Transforms = transforms.Compose([transforms.ToTensor()])\n",
        "        img = Transforms(img)\n",
        "        mask = torch.from_numpy(mask).long()\n",
        "\n",
        "        return img, mask, (h, w)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imagePaths)"
      ],
      "metadata": {
        "id": "uwEhxUR50RQN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model from L-U-Net-based"
      ],
      "metadata": {
        "id": "pEjOxvRe4Zq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dil_block(in_c, out_c):\n",
        "    conv = nn.Sequential(\n",
        "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1, dilation=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1, dilation=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "\n",
        "        )\n",
        "    return conv\n",
        "\n",
        "\n",
        "def encoding_block(in_c, out_c):\n",
        "    conv = nn.Sequential(\n",
        "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        )\n",
        "    return conv\n",
        "\n",
        "def encoding_block1(in_c, out_c):\n",
        "    conv = nn.Sequential(\n",
        "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        )\n",
        "    return conv\n",
        "\n",
        "class unet_model(nn.Module):\n",
        "    def __init__(self,out_channels=4,features=[16, 32]):\n",
        "        super(unet_model,self).__init__()\n",
        "\n",
        "\n",
        "        self.dil1 = dil_block(3,features[0])\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
        "\n",
        "        self.dil2 = dil_block(features[0],features[0])\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
        "\n",
        "        self.dil3 = dil_block(features[0],features[0])\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
        "\n",
        "        self.dil4 = dil_block(features[0],features[0])\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
        "\n",
        "        self.bott = encoding_block1(features[0],features[0])\n",
        "\n",
        "        self.tconv1 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv1 = encoding_block(features[1],features[0])\n",
        "\n",
        "        self.tconv2 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = encoding_block(features[1],features[0])\n",
        "\n",
        "        self.tconv3 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = encoding_block(features[1],features[0])\n",
        "\n",
        "        self.tconv4 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4 = encoding_block1(features[1],features[0])\n",
        "\n",
        "        self.final_layer = nn.Conv2d(features[0],out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        dil_1 = self.dil1(x)\n",
        "\n",
        "        pool_1 = self.pool1(dil_1)\n",
        "\n",
        "        dil_2 = self.dil2(pool_1)\n",
        "\n",
        "        pool_2 = self.pool2(dil_2)\n",
        "\n",
        "        dil_3 = self.dil3(pool_2)\n",
        "\n",
        "        pool_3 = self.pool3(dil_3)\n",
        "\n",
        "        dil_4 = self.dil4(pool_3)\n",
        "\n",
        "        pool_4 = self.pool4(dil_4)\n",
        "\n",
        "        bott = self.bott(pool_4)\n",
        "\n",
        "        tconv_1 = self.tconv1(bott)\n",
        "\n",
        "        concat1 = torch.cat((tconv_1, dil_4), dim=1)\n",
        "\n",
        "        conv_1 = self.conv1(concat1)\n",
        "\n",
        "        tconv_2 = self.tconv2(conv_1)\n",
        "\n",
        "        concat2 = torch.cat((tconv_2, dil_3), dim=1)\n",
        "\n",
        "        conv_2 = self.conv2(concat2)\n",
        "\n",
        "        tconv_3 = self.tconv3(conv_2)\n",
        "\n",
        "        concat3 = torch.cat((tconv_3, dil_2), dim=1)\n",
        "\n",
        "        conv_3 = self.conv3(concat3)\n",
        "\n",
        "        tconv_4 = self.tconv4(conv_3)\n",
        "\n",
        "        concat4 = torch.cat((tconv_4, dil_1), dim=1)\n",
        "\n",
        "        conv_4 = self.conv4(concat4)\n",
        "\n",
        "        x = self.final_layer(conv_4)\n",
        "\n",
        "        return x\n",
        "\n",
        "class finetuning_unet_model(nn.Module):\n",
        "    def __init__(self, unet_model, out_channels=10, features=[16, 32]):\n",
        "        super(finetuning_unet_model,self).__init__()\n",
        "        self.unet_model = unet_model\n",
        "        self.unet_model.final_layer = nn.Conv2d(features[0],out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.unet_model(x)\n"
      ],
      "metadata": {
        "id": "b2MVWRYRytKw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_name = 'Latin16746FS'\n",
        "img_DIR = f'/content/icdar_2024_SAM/U-DIADS-Bib-FS/{collection_name}/img-{collection_name}/'\n",
        "mask_DIR = f'/content/icdar_2024_SAM/U-DIADS-Bib-FS/{collection_name}/pixel-level-gt-{collection_name}/'\n",
        "# Load training and validation data\n",
        "x_train_dir = os.path.join(img_DIR, 'training')\n",
        "y_train_dir = os.path.join(mask_DIR, 'training')\n",
        "\n",
        "x_valid_dir = os.path.join(img_DIR, 'validation')\n",
        "y_valid_dir = os.path.join(mask_DIR, 'validation')\n",
        "\n",
        "train_img_paths = glob.glob(os.path.join(x_train_dir, \"*.jpg\"))\n",
        "train_mask_paths = glob.glob(os.path.join(y_train_dir, \"*.png\"))\n",
        "val_img_paths = glob.glob(os.path.join(x_valid_dir, \"*.jpg\"))\n",
        "val_mask_paths = glob.glob(os.path.join(y_valid_dir, \"*.png\"))\n",
        "train_img_paths.sort()\n",
        "train_mask_paths.sort()\n",
        "val_img_paths.sort()\n",
        "val_mask_paths.sort()\n",
        "print('the number of image/label in the train: ',len(os.listdir(x_train_dir)))\n",
        "print('the number of image/label in the validation: ',len(os.listdir(x_valid_dir)))"
      ],
      "metadata": {
        "id": "RkR0wcl-3nBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44aa838c-c944-48a2-956f-8ae55a2fc037"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of image/label in the train:  3\n",
            "the number of image/label in the validation:  10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "7ma1JWA14cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute weight of each class\n",
        "tmp_dataset = UDIADS_Validation(train_img_paths,train_mask_paths)\n",
        "list_gt= []\n",
        "for i in range(3):\n",
        "    img, mask, (h,w)= tmp_dataset[i]\n",
        "    list_gt.extend(mask.flatten().tolist())\n",
        "compute_class_weight(class_weight=\"balanced\", classes=np.unique(list_gt), y=list_gt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmUVIOdb5QQJ",
        "outputId": "e6569b37-e349-4668-aea8-c780bca2df6d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.18784916, 35.34627426,  6.26214292,  2.4999391 , 50.95926274,\n",
              "       14.50002676])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save checkpoint\n",
        "save_ckpt = f'ckpt_finetune_{collection_name}_aug'\n",
        "os.makedirs(save_ckpt,exist_ok=True)\n",
        "\n",
        "# Data loader\n",
        "\n",
        "slidingwindow=SlidingWindowCrop((512,512))\n",
        "train_dataset = UDIADS(train_img_paths, train_mask_paths, slidingwindow)\n",
        "valid_dataset = UDIADS_Validation(val_img_paths, val_mask_paths)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, sampler=get_sampler(train_dataset), num_workers=10)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=10)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"gpu\"\n",
        "pretrained_model = unet_model().to(device)\n",
        "\n",
        "\n",
        "# Define model\n",
        "model = finetuning_unet_model(pretrained_model, out_channels=6)\n",
        "model = model.to(device)\n",
        "print('number of trainable parameters: ',sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "# Optimazation\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-05)\n",
        "scheduler = StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "\n",
        "# Compute from training data\n",
        "weights = torch.tensor([0.4, 11, 3, 1.7, 4.7, 3]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weights)\n",
        "# Training\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "train_f1 = []\n",
        "val_f1 = []\n",
        "train_IoU = []\n",
        "val_IoU = []\n",
        "best_loss = np.Inf\n",
        "best_f1_score = 0.0\n",
        "epochs = 200\n",
        "fit_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print('Epoch: [{}/{}]'.format(epoch+1, epochs))\n",
        "\n",
        "    trainloss = 0\n",
        "    train_f1_score = 0\n",
        "    trainIoU = 0\n",
        "\n",
        "    since = time.time()\n",
        "    model.train()\n",
        "    for index, batch  in enumerate(train_loader):\n",
        "        img, label, (h, w) = batch\n",
        "        '''\n",
        "            Traning the Model.\n",
        "        '''\n",
        "        optimizer.zero_grad()\n",
        "        img = img.float()\n",
        "        img = img.squeeze(dim=0)\n",
        "        label = label.squeeze(dim=0)\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        output = model(img)\n",
        "        preds = torch.argmax(output, 1)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        trainloss+=loss.item()\n",
        "        train_f1_score += f1_score_metric(preds, label)\n",
        "        trainIoU += mIoU(output, label,n_classes=6)\n",
        "    scheduler.step()\n",
        "\n",
        "    print('Epoch:', epoch+1, 'LR:', scheduler.get_last_lr()[0])\n",
        "    model.eval()\n",
        "    valloss = 0\n",
        "    val_f1_score = 0\n",
        "    valIoU = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for img_val, label_val, (h, w) in valid_loader:\n",
        "        '''\n",
        "            Validation of Model.\n",
        "        '''\n",
        "        img_val=img_val.float()\n",
        "        img_val = img_val.to(device)\n",
        "        label_val = label_val.to(device)\n",
        "        output_val = model(img_val)\n",
        "        preds_val = torch.argmax(output_val, 1)\n",
        "        loss_val = criterion(output_val,label_val)\n",
        "\n",
        "        valloss+=loss_val.item()\n",
        "        val_f1_score += f1_score_metric(preds_val, label_val)\n",
        "        valIoU += mIoU(output_val, label_val,n_classes=6)\n",
        "\n",
        "    train_loss.append(trainloss/len(train_loader))\n",
        "    train_f1.append(train_f1_score/len(train_loader))\n",
        "    train_IoU.append(trainIoU/len(train_loader))\n",
        "    val_loss.append(valloss/len(valid_loader))\n",
        "    val_f1.append(val_f1_score/len(valid_loader))\n",
        "    val_IoU.append(valIoU/len(valid_loader))\n",
        "\n",
        "    # Save model if a better val IoU score is obtained\n",
        "    if best_loss > valloss:\n",
        "         best_loss = valloss\n",
        "         torch.save({\n",
        "            'epoch': epochs,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': criterion,\n",
        "            }, f'ckpt_finetune_{collection_name}_aug/best_val_loss_512x512.pth')\n",
        "         print('Loss_Model saved!')\n",
        "\n",
        "    # Save model if a better val IoU score is obtained\n",
        "    if best_f1_score < val_f1_score:\n",
        "         best_f1_score = val_f1_score\n",
        "         torch.save({\n",
        "            'epoch': epochs,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': criterion,\n",
        "            }, f'ckpt_finetune_{collection_name}_aug/best_val_f1score_512x512.pth')\n",
        "         print('IOU_Model saved!')\n",
        "\n",
        "    #print(\"epoch : {} ,train loss : {} ,valid loss : {} ,train acc : {} ,val acc : {} \".format(i,train_loss[-1],val_loss[-1],train_accuracy[-1],val_accuracy[-1]))\n",
        "    print(#\"Epoch:{}\".format(epoch),\n",
        "          \"Train Loss: {}\".format(trainloss/len(train_loader)),\n",
        "          \"Val Loss: {}\".format(valloss/len(valid_loader)),\n",
        "          \"Train mIoU:{}\".format(trainIoU/len(train_loader)),\n",
        "          \"Val mIoU: {}\".format(valIoU/len(valid_loader)),\n",
        "          \"Train F1:{}\".format(train_f1_score/len(train_loader)),\n",
        "          \"Val F1:{}\".format(val_f1_score/len(valid_loader)),\n",
        "          \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
        "print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))"
      ],
      "metadata": {
        "id": "3-jTv7Ke3uBN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P9vnOeqZAHQW"
      }
    }
  ]
}