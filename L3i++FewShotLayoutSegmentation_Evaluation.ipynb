{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honghanhh/icdar_2024_SAM/blob/main/L3i%2B%2BFewShotLayoutSegmentation_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jDSGHr5rck4z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import glob\n",
        "from skimage.filters import threshold_sauvola"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCp2dtQfdbp2",
        "outputId": "69e97003-2b82-4156-b5bf-fc04f388ccbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf icdar_2024_SAM\n",
        "!git clone https://github.com/honghanhh/icdar_2024_SAM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIyXTVXodnCX",
        "outputId": "d39ab4b0-b154-4a78-f377-8ebb6416ab69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'icdar_2024_SAM'...\n",
            "remote: Enumerating objects: 261, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 261 (delta 6), reused 13 (delta 4), pack-reused 242\u001b[K\n",
            "Receiving objects: 100% (261/261), 44.64 MiB | 13.79 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some utility function for process data"
      ],
      "metadata": {
        "id": "_yxBuNtQe8Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convertRGB_to_label(image):\n",
        "    \"\"\"\n",
        "    Convert RGB image to mask label\n",
        "    \"\"\"\n",
        "    # Define RGB color values\n",
        "    colors = {\n",
        "        (0, 0, 0): \"Background\",\n",
        "        (255, 255, 0): \"Paratext\",\n",
        "        (0, 255, 255): \"Decoration\",\n",
        "        (255, 0, 255): \"Main Text\",\n",
        "        (255, 0, 0): \"Title\",\n",
        "        (0, 255, 0): \"Chapter Headings\"\n",
        "    }\n",
        "\n",
        "    # Convert image to numpy array if it's not already\n",
        "    image = np.array(image)\n",
        "\n",
        "    # Convert image to 3D if it's grayscale\n",
        "    if len(image.shape) == 2:\n",
        "        image = np.stack((image,) * 3, axis=-1)\n",
        "\n",
        "    # Initialize labels array with the same shape as the input image\n",
        "    labels = np.zeros_like(image[:, :, 0], dtype=np.int8)\n",
        "\n",
        "    # Assign labels based on color\n",
        "    for color, label in colors.items():\n",
        "        mask = np.all(image == np.array(color), axis=-1)\n",
        "        labels[mask] = list(colors.values()).index(label)\n",
        "\n",
        "    return labels\n",
        "\n",
        "def convertLabel_to_RGB(labels):\n",
        "    \"\"\"\n",
        "    Convert mask image to mask RGB\n",
        "    \"\"\"\n",
        "    label_colors = {\n",
        "        0: [0, 0, 0],            # Background\n",
        "        1: [255, 255, 0],        # Paratext\n",
        "        2: [0, 255, 255],        # Decoration\n",
        "        3: [255, 0, 255],        # Main Text\n",
        "        4: [255, 0, 0],          # Title\n",
        "        5: [0, 255, 0]           # Chapter Headings\n",
        "    }\n",
        "\n",
        "    # Create an empty RGB image with the same shape as labels\n",
        "    h, w = labels.shape\n",
        "    rgb_image = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "\n",
        "    # Assign colors based on label values\n",
        "    for label_value, color in label_colors.items():\n",
        "        mask = labels == label_value\n",
        "        rgb_image[mask] = color\n",
        "\n",
        "    return rgb_image\n"
      ],
      "metadata": {
        "id": "Jd9-MW4ve1u7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "vGDb5gt1ekAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UDIADS_Validation(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for simple Evaluation and Testing\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            imagePaths,\n",
        "            maskPaths,\n",
        "\n",
        "    ):\n",
        "        self.imagePaths = imagePaths\n",
        "        self.maskPaths = maskPaths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # read data\n",
        "        img_path = self.imagePaths[idx]\n",
        "        mask_path = self.maskPaths[idx]\n",
        "        read_img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        h, w = read_img.shape[0], read_img.shape[1]\n",
        "        img = 2*((read_img - read_img.min()) / (read_img.max() - read_img.min())) - 1\n",
        "        mask = cv2.imread(mask_path)\n",
        "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = convertRGB_to_label(mask)\n",
        "\n",
        "\n",
        "        #To tensor\n",
        "        Transforms = transforms.Compose([transforms.ToTensor()])\n",
        "        img = Transforms(img)\n",
        "        mask = torch.from_numpy(mask).long()\n",
        "\n",
        "        return img, mask, (h, w)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imagePaths)"
      ],
      "metadata": {
        "id": "8d9wTg7kem4s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "cSkYd9dPeVCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dil_block(in_c, out_c):\n",
        "    conv = nn.Sequential(\n",
        "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1, dilation=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1, dilation=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "\n",
        "        )\n",
        "    return conv\n",
        "\n",
        "\n",
        "def encoding_block(in_c, out_c):\n",
        "    conv = nn.Sequential(\n",
        "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        )\n",
        "    return conv\n",
        "\n",
        "def encoding_block1(in_c, out_c):\n",
        "    conv = nn.Sequential(\n",
        "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        )\n",
        "    return conv\n",
        "\n",
        "class unet_model(nn.Module):\n",
        "    def __init__(self,out_channels=4,features=[16, 32]):\n",
        "        super(unet_model,self).__init__()\n",
        "\n",
        "\n",
        "        self.dil1 = dil_block(3,features[0])\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
        "\n",
        "        self.dil2 = dil_block(features[0],features[0])\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
        "\n",
        "        self.dil3 = dil_block(features[0],features[0])\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
        "\n",
        "        self.dil4 = dil_block(features[0],features[0])\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
        "\n",
        "        self.bott = encoding_block1(features[0],features[0])\n",
        "\n",
        "        self.tconv1 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv1 = encoding_block(features[1],features[0])\n",
        "\n",
        "        self.tconv2 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = encoding_block(features[1],features[0])\n",
        "\n",
        "        self.tconv3 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = encoding_block(features[1],features[0])\n",
        "\n",
        "        self.tconv4 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4 = encoding_block1(features[1],features[0])\n",
        "\n",
        "        self.final_layer = nn.Conv2d(features[0],out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        dil_1 = self.dil1(x)\n",
        "\n",
        "        pool_1 = self.pool1(dil_1)\n",
        "\n",
        "        dil_2 = self.dil2(pool_1)\n",
        "\n",
        "        pool_2 = self.pool2(dil_2)\n",
        "\n",
        "        dil_3 = self.dil3(pool_2)\n",
        "\n",
        "        pool_3 = self.pool3(dil_3)\n",
        "\n",
        "        dil_4 = self.dil4(pool_3)\n",
        "\n",
        "        pool_4 = self.pool4(dil_4)\n",
        "\n",
        "        bott = self.bott(pool_4)\n",
        "\n",
        "        tconv_1 = self.tconv1(bott)\n",
        "\n",
        "        concat1 = torch.cat((tconv_1, dil_4), dim=1)\n",
        "\n",
        "        conv_1 = self.conv1(concat1)\n",
        "\n",
        "        tconv_2 = self.tconv2(conv_1)\n",
        "\n",
        "        concat2 = torch.cat((tconv_2, dil_3), dim=1)\n",
        "\n",
        "        conv_2 = self.conv2(concat2)\n",
        "\n",
        "        tconv_3 = self.tconv3(conv_2)\n",
        "\n",
        "        concat3 = torch.cat((tconv_3, dil_2), dim=1)\n",
        "\n",
        "        conv_3 = self.conv3(concat3)\n",
        "\n",
        "        tconv_4 = self.tconv4(conv_3)\n",
        "\n",
        "        concat4 = torch.cat((tconv_4, dil_1), dim=1)\n",
        "\n",
        "        conv_4 = self.conv4(concat4)\n",
        "\n",
        "        x = self.final_layer(conv_4)\n",
        "\n",
        "        return x\n",
        "\n",
        "class finetuning_unet_model(nn.Module):\n",
        "    def __init__(self, unet_model, out_channels=10, features=[16, 32]):\n",
        "        super(finetuning_unet_model,self).__init__()\n",
        "        self.unet_model = unet_model\n",
        "        self.unet_model.final_layer = nn.Conv2d(features[0],out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.unet_model(x)"
      ],
      "metadata": {
        "id": "1Y0OmmybeWzm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_name ='Latin16746FS'\n",
        "w_ = 256\n",
        "h_ = 256"
      ],
      "metadata": {
        "id": "Fr7MZAtJdsds"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_DIR = f'/content/icdar_2024_SAM/U-DIADS-Bib-FS/{collection_name}/img-{collection_name}/'\n",
        "mask_DIR = f'/content/icdar_2024_SAM/U-DIADS-Bib-FS/{collection_name}/pixel-level-gt-{collection_name}/'\n",
        "x_valid_dir = os.path.join(img_DIR, 'validation')\n",
        "y_valid_dir = os.path.join(mask_DIR, 'validation')\n",
        "val_img_paths = glob.glob(os.path.join(x_valid_dir, \"*.jpg\"))\n",
        "val_mask_paths = glob.glob(os.path.join(y_valid_dir, \"*.png\"))\n",
        "val_img_paths.sort()\n",
        "val_mask_paths.sort()\n",
        "print(val_img_paths[:5])\n",
        "print(val_mask_paths[:5])\n",
        "\n",
        "valid_dataset = UDIADS_Validation(\n",
        "    val_img_paths,\n",
        "    val_mask_paths,\n",
        "\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "pretrained_model = unet_model().to(device)\n",
        "model = finetuning_unet_model(pretrained_model,out_channels=6)\n",
        "pretrained_ckpt = torch.load(f'/content/icdar_2024_SAM/checkpoints/ckpt_finetune_{collection_name}/best_val_f1score_{h_}x{w_}.pth')\n",
        "# load model weights state_dict\n",
        "model.load_state_dict(pretrained_ckpt['model_state_dict'])\n",
        "model.eval()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0vq7KD_d85I",
        "outputId": "0c935b65-83b9-42e5-f59a-9a6ec9316423"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin16746FS/img-Latin16746FS/validation/024.jpg', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin16746FS/img-Latin16746FS/validation/107.jpg', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin16746FS/img-Latin16746FS/validation/116.jpg', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin16746FS/img-Latin16746FS/validation/122.jpg', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin16746FS/img-Latin16746FS/validation/147.jpg']\n",
            "['/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin16746FS/pixel-level-gt-Latin16746FS/validation/024.png', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin16746FS/pixel-level-gt-Latin16746FS/validation/107.png', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin16746FS/pixel-level-gt-Latin16746FS/validation/116.png', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin16746FS/pixel-level-gt-Latin16746FS/validation/122.png', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin16746FS/pixel-level-gt-Latin16746FS/validation/147.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CODE to generate output image"
      ],
      "metadata": {
        "id": "L1nuUI84pwpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = '/content/icdar_2024_SAM/U-DIADS-Bib-FS/'\n",
        "#for DS in [\"Latin2FS\", \"Latin14396FS\", \"Latin16746FS\", \"Syr341FS\"]:\n",
        "for DS in [\"Latin16746FS\"]:\n",
        "    out_results = f'Unet-based/{DS}/result-{h_}x{w_}'\n",
        "    os.makedirs(out_results,exist_ok=True)\n",
        "    current_path = os.path.join(test_dir,DS,'img-'+DS,'validation')\n",
        "\n",
        "    list_img = glob.glob(current_path+'/*')\n",
        "    for im in list_img:\n",
        "        img  = cv2.cvtColor(cv2.imread(im), cv2.COLOR_BGR2RGB)\n",
        "        shape = img.shape\n",
        "        h, w = img.shape[0], img.shape[1]\n",
        "        img = 2*((img - img.min()) / (img.max() - img.min())) - 1\n",
        "        Transforms = transforms.Compose([transforms.ToTensor()])\n",
        "        img = Transforms(img)\n",
        "        inputs = img.to(device).unsqueeze(0)\n",
        "        outputs = model(inputs.float())\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        t = preds.cpu()\n",
        "        t = torch.transpose(t, 0, 1).transpose(1, 2)\n",
        "        t_np = t.numpy()[:,:,0]\n",
        "        rgb_image = convertLabel_to_RGB(t_np)\n",
        "        rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
        "        # if test with Latin16746FS, run the following postprocessing step\n",
        "        # otherwise, comment it\n",
        "        gray_img = cv2.imread(im,cv2.IMREAD_GRAYSCALE)\n",
        "        thresh_ = threshold_sauvola(gray_img, window_size=25)\n",
        "        bin_img = (gray_img > thresh_).astype(np.uint8) * 255\n",
        "        rgb_image[bin_img==255]=0\n",
        "        cv2.imwrite(os.path.join(out_results,os.path.basename(im)[0:-4]+'.png'),rgb_image)"
      ],
      "metadata": {
        "id": "IPHvZdZbghK6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute evaluation metrics"
      ],
      "metadata": {
        "id": "LaXS7RlejMtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/icdar_2024_SAM/Unet-based/metric.py"
      ],
      "metadata": {
        "id": "n-bsT0T1jQm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be22ca5-551a-4628-ac9f-d689e76048bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############## Latin14396FS Scores ##############\n",
            "Precision:  0.8987766214915855\n",
            "Recall:  0.9807385430738644\n",
            "F1 score:  0.9351480699325356\n",
            "Intersection Over Union:  0.8832217572127345\n",
            "############## Syr341FS Scores ##############\n",
            "Precision:  0.8658239115498026\n",
            "Recall:  0.9663057854370031\n",
            "F1 score:  0.9083066809791462\n",
            "Intersection Over Union:  0.8406053073668961\n",
            "############## Latin2FS Scores ##############\n",
            "Precision:  0.8667847410296519\n",
            "Recall:  0.9669577086926089\n",
            "F1 score:  0.9094735542039508\n",
            "Intersection Over Union:  0.8429133151410223\n",
            "############## Latin16746FS Scores ##############\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Precision:  0.19691445236409105\n",
            "Recall:  0.21272169813690678\n",
            "F1 score:  0.20400796468778884\n",
            "Intersection Over Union:  0.1902492325682581\n",
            "############## Final Scores ##############\n",
            "Final result of Intersection Over Union:  0.6892474030722278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lLEV5NAQ8Nn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}