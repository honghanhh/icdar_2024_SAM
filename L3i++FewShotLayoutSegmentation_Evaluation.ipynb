{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/honghanhh/icdar_2024_SAM/blob/main/L3i%2B%2BFewShotLayoutSegmentation_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDSGHr5rck4z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "from skimage.filters import threshold_sauvola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCp2dtQfdbp2",
    "outputId": "014d6191-996b-4fdb-81fa-5b5420259a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIyXTVXodnCX",
    "outputId": "f117743f-4497-43c0-fa44-9d71ddb0f292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'icdar_2024_SAM'...\n",
      "remote: Enumerating objects: 278, done.\u001b[K\n",
      "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
      "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
      "remote: Total 278 (delta 16), reused 10 (delta 4), pack-reused 242\u001b[K\n",
      "Receiving objects: 100% (278/278), 44.66 MiB | 35.47 MiB/s, done.\n",
      "Resolving deltas: 100% (36/36), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf icdar_2024_SAM\n",
    "!git clone https://github.com/honghanhh/icdar_2024_SAM.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yxBuNtQe8Rx"
   },
   "source": [
    "# Some utility function for process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jd9-MW4ve1u7"
   },
   "outputs": [],
   "source": [
    "def convertRGB_to_label(image):\n",
    "    \"\"\"\n",
    "    Convert RGB image to mask label\n",
    "    \"\"\"\n",
    "    # Define RGB color values\n",
    "    colors = {\n",
    "        (0, 0, 0): \"Background\",\n",
    "        (255, 255, 0): \"Paratext\",\n",
    "        (0, 255, 255): \"Decoration\",\n",
    "        (255, 0, 255): \"Main Text\",\n",
    "        (255, 0, 0): \"Title\",\n",
    "        (0, 255, 0): \"Chapter Headings\"\n",
    "    }\n",
    "\n",
    "    # Convert image to numpy array if it's not already\n",
    "    image = np.array(image)\n",
    "\n",
    "    # Convert image to 3D if it's grayscale\n",
    "    if len(image.shape) == 2:\n",
    "        image = np.stack((image,) * 3, axis=-1)\n",
    "\n",
    "    # Initialize labels array with the same shape as the input image\n",
    "    labels = np.zeros_like(image[:, :, 0], dtype=np.int8)\n",
    "\n",
    "    # Assign labels based on color\n",
    "    for color, label in colors.items():\n",
    "        mask = np.all(image == np.array(color), axis=-1)\n",
    "        labels[mask] = list(colors.values()).index(label)\n",
    "\n",
    "    return labels\n",
    "\n",
    "def convertLabel_to_RGB(labels):\n",
    "    \"\"\"\n",
    "    Convert mask image to mask RGB\n",
    "    \"\"\"\n",
    "    label_colors = {\n",
    "        0: [0, 0, 0],            # Background\n",
    "        1: [255, 255, 0],        # Paratext\n",
    "        2: [0, 255, 255],        # Decoration\n",
    "        3: [255, 0, 255],        # Main Text\n",
    "        4: [255, 0, 0],          # Title\n",
    "        5: [0, 255, 0]           # Chapter Headings\n",
    "    }\n",
    "\n",
    "    # Create an empty RGB image with the same shape as labels\n",
    "    h, w = labels.shape\n",
    "    rgb_image = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    # Assign colors based on label values\n",
    "    for label_value, color in label_colors.items():\n",
    "        mask = labels == label_value\n",
    "        rgb_image[mask] = color\n",
    "\n",
    "    return rgb_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGDb5gt1ekAw"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d9wTg7kem4s"
   },
   "outputs": [],
   "source": [
    "class UDIADS_Validation(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for simple Evaluation and Testing\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            imagePaths,\n",
    "            maskPaths,\n",
    "\n",
    "    ):\n",
    "        self.imagePaths = imagePaths\n",
    "        self.maskPaths = maskPaths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # read data\n",
    "        img_path = self.imagePaths[idx]\n",
    "        mask_path = self.maskPaths[idx]\n",
    "        read_img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        h, w = read_img.shape[0], read_img.shape[1]\n",
    "        img = 2*((read_img - read_img.min()) / (read_img.max() - read_img.min())) - 1\n",
    "        mask = cv2.imread(mask_path)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = convertRGB_to_label(mask)\n",
    "\n",
    "\n",
    "        #To tensor\n",
    "        Transforms = transforms.Compose([transforms.ToTensor()])\n",
    "        img = Transforms(img)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        return img, mask, (h, w)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagePaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSkYd9dPeVCC"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Y0OmmybeWzm"
   },
   "outputs": [],
   "source": [
    "def dil_block(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1, dilation=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1, dilation=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=2, dilation=2),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=2, dilation=2),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "\n",
    "        )\n",
    "    return conv\n",
    "\n",
    "\n",
    "def encoding_block(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        )\n",
    "    return conv\n",
    "\n",
    "def encoding_block1(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        )\n",
    "    return conv\n",
    "\n",
    "class unet_model(nn.Module):\n",
    "    def __init__(self,out_channels=4,features=[16, 32]):\n",
    "        super(unet_model,self).__init__()\n",
    "\n",
    "\n",
    "        self.dil1 = dil_block(3,features[0])\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
    "\n",
    "        self.dil2 = dil_block(features[0],features[0])\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
    "\n",
    "        self.dil3 = dil_block(features[0],features[0])\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
    "\n",
    "        self.dil4 = dil_block(features[0],features[0])\n",
    "\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
    "\n",
    "        self.bott = encoding_block1(features[0],features[0])\n",
    "\n",
    "        self.tconv1 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv1 = encoding_block(features[1],features[0])\n",
    "\n",
    "        self.tconv2 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = encoding_block(features[1],features[0])\n",
    "\n",
    "        self.tconv3 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = encoding_block(features[1],features[0])\n",
    "\n",
    "        self.tconv4 = nn.ConvTranspose2d(features[0], features[0], kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = encoding_block1(features[1],features[0])\n",
    "\n",
    "        self.final_layer = nn.Conv2d(features[0],out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        dil_1 = self.dil1(x)\n",
    "\n",
    "        pool_1 = self.pool1(dil_1)\n",
    "\n",
    "        dil_2 = self.dil2(pool_1)\n",
    "\n",
    "        pool_2 = self.pool2(dil_2)\n",
    "\n",
    "        dil_3 = self.dil3(pool_2)\n",
    "\n",
    "        pool_3 = self.pool3(dil_3)\n",
    "\n",
    "        dil_4 = self.dil4(pool_3)\n",
    "\n",
    "        pool_4 = self.pool4(dil_4)\n",
    "\n",
    "        bott = self.bott(pool_4)\n",
    "\n",
    "        tconv_1 = self.tconv1(bott)\n",
    "\n",
    "        concat1 = torch.cat((tconv_1, dil_4), dim=1)\n",
    "\n",
    "        conv_1 = self.conv1(concat1)\n",
    "\n",
    "        tconv_2 = self.tconv2(conv_1)\n",
    "\n",
    "        concat2 = torch.cat((tconv_2, dil_3), dim=1)\n",
    "\n",
    "        conv_2 = self.conv2(concat2)\n",
    "\n",
    "        tconv_3 = self.tconv3(conv_2)\n",
    "\n",
    "        concat3 = torch.cat((tconv_3, dil_2), dim=1)\n",
    "\n",
    "        conv_3 = self.conv3(concat3)\n",
    "\n",
    "        tconv_4 = self.tconv4(conv_3)\n",
    "\n",
    "        concat4 = torch.cat((tconv_4, dil_1), dim=1)\n",
    "\n",
    "        conv_4 = self.conv4(concat4)\n",
    "\n",
    "        x = self.final_layer(conv_4)\n",
    "\n",
    "        return x\n",
    "\n",
    "class finetuning_unet_model(nn.Module):\n",
    "    def __init__(self, unet_model, out_channels=10, features=[16, 32]):\n",
    "        super(finetuning_unet_model,self).__init__()\n",
    "        self.unet_model = unet_model\n",
    "        self.unet_model.final_layer = nn.Conv2d(features[0],out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.unet_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fr7MZAtJdsds"
   },
   "outputs": [],
   "source": [
    "collection_name ='Latin14396FS'\n",
    "w_ = 512\n",
    "h_ = 512\n",
    "# the other dataset use 256x256, update ckpt of Latin14396 _aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0vq7KD_d85I",
    "outputId": "923cb5ac-d60b-4bf4-af90-abb33a52526f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin14396FS/img-Latin14396FS/validation/028.jpg', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin14396FS/img-Latin14396FS/validation/040.jpg', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin14396FS/img-Latin14396FS/validation/044.jpg', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin14396FS/img-Latin14396FS/validation/064.jpg', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin14396FS/img-Latin14396FS/validation/137.jpg']\n",
      "['/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin14396FS/pixel-level-gt-Latin14396FS/validation/028.png', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin14396FS/pixel-level-gt-Latin14396FS/validation/040.png', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin14396FS/pixel-level-gt-Latin14396FS/validation/044.png', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin14396FS/pixel-level-gt-Latin14396FS/validation/064.png', '/content/icdar_2024_SAM/U-DIADS-Bib-FS/Latin14396FS/pixel-level-gt-Latin14396FS/validation/137.png']\n"
     ]
    }
   ],
   "source": [
    "img_DIR = f'/content/icdar_2024_SAM/U-DIADS-Bib-FS/{collection_name}/img-{collection_name}/'\n",
    "mask_DIR = f'/content/icdar_2024_SAM/U-DIADS-Bib-FS/{collection_name}/pixel-level-gt-{collection_name}/'\n",
    "x_valid_dir = os.path.join(img_DIR, 'validation')\n",
    "y_valid_dir = os.path.join(mask_DIR, 'validation')\n",
    "val_img_paths = glob.glob(os.path.join(x_valid_dir, \"*.jpg\"))\n",
    "val_mask_paths = glob.glob(os.path.join(y_valid_dir, \"*.png\"))\n",
    "val_img_paths.sort()\n",
    "val_mask_paths.sort()\n",
    "print(val_img_paths[:5])\n",
    "print(val_mask_paths[:5])\n",
    "\n",
    "valid_dataset = UDIADS_Validation(\n",
    "    val_img_paths,\n",
    "    val_mask_paths,\n",
    "\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pretrained_model = unet_model().to(device)\n",
    "model = finetuning_unet_model(pretrained_model,out_channels=6)\n",
    "pretrained_ckpt = torch.load(f'/content/icdar_2024_SAM/checkpoints/ckpt_finetune_{collection_name}/best_val_f1score_{h_}x{w_}.pth')\n",
    "# load model weights state_dict\n",
    "model.load_state_dict(pretrained_ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1nuUI84pwpG"
   },
   "source": [
    "# CODE to generate output image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1PdPTfgxlh5"
   },
   "source": [
    "# Post processing Latin16746FS\n",
    "**bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbrEdfQ4wZty"
   },
   "outputs": [],
   "source": [
    "def find_connected_components(image):\n",
    "    # Convert the image to binary\n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find connected components\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary_image, connectivity=8)\n",
    "\n",
    "    # Filter out the background component\n",
    "    stats = stats[1:]\n",
    "\n",
    "    return num_labels - 1, labels, stats\n",
    "\n",
    "def keep_largest_component(image):\n",
    "    num_labels, labels, stats = find_connected_components(image)\n",
    "\n",
    "    # Sort by area\n",
    "    sorted_stats = sorted(stats, key=lambda x: -x[4])\n",
    "\n",
    "    largest_area = sorted_stats[0][4]\n",
    "    second_largest_area = sorted_stats[1][4] if len(sorted_stats) > 1 else 0\n",
    "\n",
    "    # Keep the largest area and the second largest if it's larger than half of the largest area\n",
    "    keep_indices = [i for i, stat in enumerate(stats) if stat[4] == largest_area or (stat[4] == second_largest_area and second_largest_area > largest_area / 2)]\n",
    "\n",
    "    # Create a mask to keep only the desired components\n",
    "    mask = np.zeros_like(labels, dtype=np.uint8)\n",
    "    for index in keep_indices:\n",
    "        mask[labels == index + 1] = 255\n",
    "\n",
    "    return mask\n",
    "def post_processing_for_Latin16746FS(rgb_image, orig_img):\n",
    "    gray_img = cv2.imread(orig_img,cv2.IMREAD_GRAYSCALE)\n",
    "    thresh_ = threshold_sauvola(gray_img, window_size=1005)\n",
    "    bin_img = (gray_img > thresh_).astype(np.uint8) * 255\n",
    "    rgb_image[bin_img==255]=0\n",
    "\n",
    "    gray = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)\n",
    "    kernel = np.ones((91, 41), np.uint8)\n",
    "    dilated = cv2.dilate(thresh,kernel)\n",
    "    removed_img = keep_largest_component(dilated)\n",
    "    #plt.imshow(removed_img)\n",
    "    #plt.show()\n",
    "    rgb_image[removed_img==0]=0\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2KOTofa1bOD"
   },
   "source": [
    "# Post processing for Latin2FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxVst0hT1eXT"
   },
   "outputs": [],
   "source": [
    "def post_processing_for_Latin2FS(rgb_image, orig_img):\n",
    "    gray_img = cv2.imread(orig_img,cv2.IMREAD_GRAYSCALE)\n",
    "    thresh_ = threshold_sauvola(gray_img, window_size=1005)\n",
    "    bin_img = (gray_img > thresh_).astype(np.uint8) * 255\n",
    "    rgb_image[bin_img==255]=0\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2B4h_UQ3p2n"
   },
   "source": [
    "# Post processing for Syr341FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXz-yga23u7S"
   },
   "outputs": [],
   "source": [
    "def imclearborder(img):\n",
    "    '''\n",
    "    Remove targets in binary images that are in contact with edges\n",
    "    @param img: numpy.array, source image, must be a binary image\n",
    "    @return cropImg: numpy.array, image without border tagets\n",
    "    '''\n",
    "    h, w = img.shape\n",
    "    # expand binary image with a white border of thickness 10\n",
    "    x = 10\n",
    "    extended = cv2.copyMakeBorder(img, x, x, x, x, cv2.BORDER_CONSTANT, value=255)\n",
    "\n",
    "    # Then fill the white border with black\n",
    "    mh, mw = extended.shape[:2]\n",
    "    mask = np.zeros([mh + 2, mw + 2], np.uint8)\n",
    "    cv2.floodFill(extended, mask, (0, 0), 0,flags=cv2.FLOODFILL_FIXED_RANGE)\n",
    "    cv2.floodFill(extended, mask, (w, h), 0,flags=cv2.FLOODFILL_FIXED_RANGE)\n",
    "    cv2.floodFill(extended, mask, (w, 0), 0,flags=cv2.FLOODFILL_FIXED_RANGE)\n",
    "    cv2.floodFill(extended, mask, (0, h), 0,flags=cv2.FLOODFILL_FIXED_RANGE)\n",
    "\n",
    "    # crop from the original position\n",
    "    cropImg = extended[x:x+h, x:x+w]\n",
    "    return cropImg\n",
    "def remove_small_components(image, min_area):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the image\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find connected components and their statistics\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(thresh, connectivity=8)\n",
    "\n",
    "    # Iterate through connected components and remove small ones\n",
    "    img_filtered = np.zeros(thresh.shape, dtype='uint8')\n",
    "    for i in range(1, num_labels):  # Exclude the background label (0)\n",
    "        if stats[i, cv2.CC_STAT_AREA] <= min_area:\n",
    "            image[labels == i] = 0\n",
    "    return image\n",
    "\n",
    "def post_processing_for_Syr341FS(rgb_image):\n",
    "    gray = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)\n",
    "    removed_img = imclearborder(thresh)\n",
    "    rgb_image[removed_img==0]=0\n",
    "    rgb_image = remove_small_components(rgb_image,50)\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7w5aqC8tVvF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqfSMQ_-tcGY"
   },
   "source": [
    "\n",
    "# Post-processing for Latin14396FS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIUerUOxtVJR"
   },
   "outputs": [],
   "source": [
    "def post_processing_for_Latin14396FS(rgb_image, orig_img):\n",
    "  gray_img = cv2.imread(orig_img,cv2.IMREAD_GRAYSCALE)\n",
    "  thresh_ = threshold_sauvola(gray_img, window_size=1005)\n",
    "  bin_img = (gray_img > thresh_).astype(np.uint8) * 255\n",
    "  rgb_image[bin_img==255]=0\n",
    "  return rgb_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-FYvif-xxCS"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPHvZdZbghK6"
   },
   "outputs": [],
   "source": [
    "test_dir = '/content/icdar_2024_SAM/U-DIADS-Bib-FS/'\n",
    "#for DS in [\"Latin2FS\", \"Latin14396FS\", \"Latin16746FS\", \"Syr341FS\"]:\n",
    "for DS in [collection_name]:\n",
    "    out_results = f'/content/icdar_2024_SAM/Unet-based/{DS}/result-{h_}x{w_}'\n",
    "    os.makedirs(out_results,exist_ok=True)\n",
    "    current_path = os.path.join(test_dir,DS,'img-'+DS,'validation')\n",
    "\n",
    "    list_img = glob.glob(current_path+'/*')\n",
    "    for im in list_img:\n",
    "        img  = cv2.cvtColor(cv2.imread(im), cv2.COLOR_BGR2RGB)\n",
    "        shape = img.shape\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        img = 2*((img - img.min()) / (img.max() - img.min())) - 1\n",
    "        Transforms = transforms.Compose([transforms.ToTensor()])\n",
    "        img = Transforms(img)\n",
    "        inputs = img.to(device).unsqueeze(0)\n",
    "        outputs = model(inputs.float())\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        t = preds.cpu()\n",
    "        t = torch.transpose(t, 0, 1).transpose(1, 2)\n",
    "        t_np = t.numpy()[:,:,0]\n",
    "        rgb_image = convertLabel_to_RGB(t_np)\n",
    "        rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "        if DS == 'Latin16746FS':\n",
    "          rgb_image = post_processing_for_Latin16746FS(rgb_image, im)\n",
    "        elif DS== 'Latin2FS':\n",
    "          rgb_image = post_processing_for_Latin2FS(rgb_image, im)\n",
    "        elif DS =='Latin14396FS':\n",
    "          rgb_image =post_processing_for_Latin14396FS(rgb_image, im)\n",
    "        else:\n",
    "          rgb_image =post_processing_for_Syr341FS(rgb_image)\n",
    "\n",
    "        cv2.imwrite(os.path.join(out_results,os.path.basename(im)[0:-4]+'.png'),rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGTMEhQbxwDd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaXS7RlejMtB"
   },
   "source": [
    "# Compute evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-bsT0T1jQm9",
    "outputId": "92360652-95a8-4c7c-cac9-25822227c1ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## Latin14396FS Scores ##############\n",
      "Precision:  0.8571748922565886\n",
      "Recall:  0.6950635384544867\n",
      "F1 score:  0.718465854224091\n",
      "Intersection Over Union:  0.6499802605482814\n",
      "The result folder is not a directory\n",
      "The result folder is not a directory\n",
      "The result folder is not a directory\n",
      "############## Final Scores ##############\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/icdar_2024_SAM/Unet-based/metric.py\", line 124, in <module>\n",
      "    print(\"Final result of Intersection Over Union: \", np.mean(result))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\", line 3504, in mean\n",
      "    return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\", line 118, in _mean\n",
      "    ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "TypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n"
     ]
    }
   ],
   "source": [
    "!python /content/icdar_2024_SAM/Unet-based/metric.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLEV5NAQ8Nn2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8r5GtesNtSCc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
