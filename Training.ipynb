{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc7a0b73-7d3d-4ba8-937c-0d4e1875b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from focal_loss import FocalLoss\n",
    "from dataloader_ICDAR import ICDAR2024, ICDAR2024_v2,SlidingWindowCrop\n",
    "from utils import get_sampler, pixel_accuracy, mIoU, f1_score_metric\n",
    "from model import finetuning_unet_model, unet_model\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303586a-8c09-48b4-b758-9e6e393a5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_list =  ['Latin14396FS', 'Latin16746FS', 'Latin2FS', 'Syr341FS']\n",
    "collection_name = 'Latin14396FS' #Choose the collection to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93efe71d-4189-41d0-94c2-8690483a3812",
   "metadata": {},
   "source": [
    "## Set path and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae4496d-30ee-4267-b23d-bda7775685a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'ckpt_finetune_{collection_name}',exist_ok=True)\n",
    "img_DIR = f'../U-DIADS-Bib-FS/{collection_name}/img-{collection_name}/'\n",
    "mask_DIR = f'../U-DIADS-Bib-FS/{collection_name}/pixel-level-gt-{collection_name}/'\n",
    "# Load training and validation data\n",
    "x_train_dir = os.path.join(img_DIR, 'training')\n",
    "y_train_dir = os.path.join(mask_DIR, 'training')\n",
    "\n",
    "x_valid_dir = os.path.join(img_DIR, 'validation')\n",
    "y_valid_dir = os.path.join(mask_DIR, 'validation')\n",
    "\n",
    "train_img_paths = glob.glob(os.path.join(x_train_dir, \"*.jpg\"))\n",
    "train_mask_paths = glob.glob(os.path.join(y_train_dir, \"*.png\"))\n",
    "val_img_paths = glob.glob(os.path.join(x_valid_dir, \"*.jpg\"))\n",
    "val_mask_paths = glob.glob(os.path.join(y_valid_dir, \"*.png\"))\n",
    "train_img_paths.sort()\n",
    "train_mask_paths.sort()\n",
    "val_img_paths.sort()\n",
    "val_mask_paths.sort()\n",
    "\n",
    "print('the number of image/label in the train: ',len(os.listdir(x_train_dir)))\n",
    "print('the number of image/label in the validation: ',len(os.listdir(x_valid_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e62a1-bcbe-4b67-ae98-ed3c263e36b8",
   "metadata": {},
   "source": [
    "## Compute weight for cross-entropy loss, which is the square of `compute_class_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45fcc0-8148-4990-bd9d-b49e02b9fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_dataset = ICDAR2024(train_img_paths,train_mask_paths)\n",
    "list_gt= []\n",
    "for i in range(3):\n",
    "    img, mask, (h,w)= xx_dataset[i]\n",
    "    list_gt.extend(mask.flatten().tolist())\n",
    "    \n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(list_gt), y=list_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3965da-258c-4245-8447-32861190cd66",
   "metadata": {},
   "source": [
    "## Training and save best val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b8d94-285a-43e8-8d8c-53f781540f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "transform=SlidingWindowCrop((256,256))\n",
    "train_dataset = ICDAR2024_v2(train_img_paths,train_mask_paths,transform)\n",
    "valid_dataset = ICDAR2024(val_img_paths,val_mask_paths)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, sampler=get_sampler(train_dataset), num_workers=10)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=10)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"gpu\"\n",
    "pretrained_model = unet_model().to(device)\n",
    "\n",
    "model = finetuning_unet_model(pretrained_model, out_channels=6)\n",
    "model = model.to(device)\n",
    "print('number of trainable parameters: ',sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# Optimazation\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-05)\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "weights = torch.tensor([0.4, 11, 3, 1.7, 7, 3]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weights)\n",
    "# Training\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "train_IoU = []\n",
    "val_IoU = []\n",
    "best_loss = np.Inf\n",
    "best_f1_score = 0.0\n",
    "epochs = 200\n",
    "fit_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: [{}/{}]'.format(epoch+1, epochs))\n",
    "\n",
    "    trainloss = 0\n",
    "    train_f1_score = 0\n",
    "    trainIoU = 0\n",
    "\n",
    "    since = time.time()\n",
    "    model.train()\n",
    "    #for img,label in tqdm(train_loader):\n",
    "    for index, batch  in enumerate(train_loader):\n",
    "        img, label, (h, w) = batch\n",
    "        #print(img.shape)\n",
    "        '''\n",
    "            Traning the Model.\n",
    "        '''\n",
    "        #print(img.shape)\n",
    "        optimizer.zero_grad()\n",
    "        img = img.float()\n",
    "        img = img.squeeze(dim=0)\n",
    "        label = label.squeeze(dim=0)\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(img)\n",
    "        preds = torch.argmax(output, 1)\n",
    "        print(torch.unique(label,return_counts=True),torch.unique(preds,return_counts=True))\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainloss+=loss.item()\n",
    "        train_f1_score += f1_score_metric(preds, label)\n",
    "        trainIoU += mIoU(output, label)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print('Epoch:', epoch+1, 'LR:', scheduler.get_last_lr()[0])\n",
    "    model.eval()\n",
    "    valloss = 0\n",
    "    val_f1_score = 0\n",
    "    valIoU = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for img_val, label_val, (h, w) in valid_loader:\n",
    "        '''\n",
    "            Validation of Model.\n",
    "        '''\n",
    "        img_val=img_val.float()\n",
    "        img_val = img_val.to(device)\n",
    "        label_val = label_val.to(device)\n",
    "        output_val = model(img_val)\n",
    "        #output_val = F.interpolate(output_val, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        #loss_val = criterion(output_val,label_val)\n",
    "        preds_val = torch.argmax(output_val, 1)\n",
    "        loss_val = criterion(output_val,label_val)\n",
    "\n",
    "        valloss+=loss_val.item()\n",
    "        val_f1_score += f1_score_metric(preds_val, label_val)\n",
    "        valIoU += mIoU(output_val, label_val)\n",
    "\n",
    "    train_loss.append(trainloss/len(train_loader))\n",
    "    train_f1.append(train_f1_score/len(train_loader))\n",
    "    train_IoU.append(trainIoU/len(train_loader))\n",
    "    val_loss.append(valloss/len(valid_loader))\n",
    "    val_f1.append(val_f1_score/len(valid_loader))\n",
    "    val_IoU.append(valIoU/len(valid_loader))\n",
    "\n",
    "    # Save model if a better val IoU score is obtained\n",
    "    if best_loss > valloss:\n",
    "         best_loss = valloss\n",
    "         torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            }, f'ckpt_finetune_{collection_name}/best_val_loss_256x256.pth')\n",
    "         print('Loss_Model saved!')\n",
    "\n",
    "    # Save model if a better val IoU score is obtained\n",
    "    if best_f1_score < val_f1_score:\n",
    "         best_f1_score = val_f1_score\n",
    "         torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion,\n",
    "            }, f'ckpt_finetune_{collection_name}/best_val_f1score_256x256.pth')\n",
    "         print('IOU_Model saved!')\n",
    "\n",
    "    #print(\"epoch : {} ,train loss : {} ,valid loss : {} ,train acc : {} ,val acc : {} \".format(i,train_loss[-1],val_loss[-1],train_accuracy[-1],val_accuracy[-1]))\n",
    "    print(#\"Epoch:{}\".format(epoch),\n",
    "          \"Train Loss: {}\".format(trainloss/len(train_loader)),\n",
    "          \"Val Loss: {}\".format(valloss/len(valid_loader)),\n",
    "          # \"Train mIoU:{}\".format(trainIoU/len(train_loader)),\n",
    "          # \"Val mIoU: {}\".format(valIoU/len(valid_loader)),\n",
    "          \"Train F1:{}\".format(train_f1_score/len(train_loader)),\n",
    "          \"Val F1:{}\".format(val_f1_score/len(valid_loader)),\n",
    "          \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
